\section{Introduction}
\label{sec:introduction}

This report describes a data mining project, inspired by novel business concept developed by Statistico in Aarhus Denmark. Statistico is a subsidiary of a company that specializes in Search Engine Optimization (\texttt{SEO}) and web development. Statistico has build a web crawler that scrapes the \texttt{HTML} of all websites in the .dk domain. The data is stored it in a Lucene \cite{lucene} index, with \texttt{Solr} \cite{solr} build on top for querying the index. Statistico's business model is to sell access to their data to primarily companies in the ecommerce domain. Examples could be access to information on the current distribution of CMS, or how widely a new JavaScript library has been adopted. A total of 600 GB of data from around 2.5 million pages are scraped every month. Currently their work is focused on optimizing the data collection, data integrity, search and storage.
The future challenge is to extract actual value of the data. This is larger task that will require work on data representation and visualization, and non the least data mining. Currently the company has several visions for what kind of information they want to make available, but no real empirical knowledge or experience with extracting useful information. This project can be seen as a pilot project for performing data mining on the data that Statistico wants to build its business on.

\paragraph{Data set}
\label{subsec:data_set}
With the aforementioned size of the data set in mind, we have decided to only work on a subset of the data. Therefore our solution includes a component that scrapes \texttt{HTML} and \texttt{HTTP-headers} from 2425 websites in the .dk domain, which we expect to be enough for our results to generalize. For each site we process 44 attributes, and get a total of 106700 values of which 2332, or 2.18\%, are missing, so we would expect these to have little to no influence on the generalization of our results. To extract the features from \texttt{HTML} data, a substantial amount of preprocessing has been done, and as will be explained in the next section, we have also chosen work around the indexing technologies used by Statistico. Without getting into details, we found that \texttt{Solr} and \texttt{Lucene} added unnecessary complexity to the preprocessing step, and we have chosen to parse the raw \texttt{HTML} files instead --- which are also stored at Statistico. It should be clear that our solution can readily be applied on the Statistico data set, with the constraint that it would take a single machine several weeks to perform the preprocessing.

\paragraph{Motivation}
In this project we have been highly motivated by the fact that we have had the chance to work on a data mining project that is relevant for a real-world company. We thank Statistico for their time involving us in their work, and the resources and time they have invested in getting the project underway.

\paragraph{Overview}
In the next section we formalize the problem at hand, and discuss our approach to the data mining process. In section \ref{sec:solution} the solution, and in particular, the preprocessing is described. In section \ref{sec:evaluation} we evaluate and report on our results. Finally in section \ref{sec:conclusion} we summarize and conclude.