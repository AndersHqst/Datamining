\section{Introduction}
\label{sec:introduction}

This report describes a data mining project, inspired by a novel business concept developed by Statistico in Aarhus Denmark. Statistico is a subsidiary of a company that specializes in Search Engine Optimization (SEO) and web development. Statistico has build a web crawler that scrapes the HTML of all websites in the \texttt{.dk} domain. The data is stored in a Lucene index, with Solr running on top for querying the index \cite{lucene, solr}. Statistico's business model is to sell access to their data to primarily companies in the e-commerce domain. Example use cases could be to gain access to information on the current distribution of content-management systems (CMS's), or how widely a new JavaScript library has been adopted. A total of 600 GB of data from around 2.5 million pages are scraped every month. Currently their work is focused on optimizing the data collection, data integrity, search and storage.

The future challenge is to extract actual value from the data. Obviously, this is large task involving aspects such as data representation, visualization and, none the least, data mining. Currently the company has several visions for what kind of information they want to make available, but have no real empirical knowledge or experience with extracting useful information. This project can be seen as a pilot project for performing data mining on the data which Statistico wants to build its business on.

%TODO \todo{Should we cut a bit down on the Statistico part?}

\subsection{Problem statement}
\label{subsec:problem_statement}

Based on Statistico's business model and data, we want to find out how well we can extract basic statistics from websites (their HTML and HTTP headers). We want to be able to give an overview of the distribution of CMS's, webservers, and SEO related figures.

In addition, we want to investigate if it is possible to derive interesting correlations or patterns in the website data and related meta-data. We will use data mining tools to experiment with different clustering algorithms, and see if we can derive any meaningful clusters in the data.

A very important measure for any website is its PageRank\footnote{PageRank is a score given to websites by Google, affecting a site's exposure on their search engine.}. We want to investigate if it is possible to predict these values based on a relatively small number of attributes, compared to the complex feature set used by Google to calculate the PageRank. It seems unlikely, or at least not intuitive, that we should be able to derive these numbers from a data mining process, which is essentially a simplification of what Google does. None the less, the results could turn out interesting.

\subsection{Data set}
\label{subsec:data_set}

With the aforementioned size of the data set in mind, we have decided to only work on a subset of the data. Therefore, our solution includes a component that scrapes HTML and HTTP headers from the most visited 2425 websites in the \texttt{.dk} domain, which we expect to be enough to gain some interesting insight. For each of these websites, we have extracted 44 attributes, thus ending up with a total of 106700 values, of which 2332 (2.18\%) are missing. We expect these missing values to have little influence on the generalization of our results. In order to extract the attributes from the websites, a substantial amount of preprocessing has been done. 

We have also chosen to work around the indexing technologies used by Statistico. Without getting into details, we found that Solr and {Lucene} added unnecessary complexity to the preprocessing step, and have therefore chosen to parse the raw {HTML} files instead\footnote{Statistico also stores the raw data, not only the index.}. It should however be clear, that many parts our solution can be readily applied to the Statistico data set, with the constraint that it would take a single machine several weeks to perform the preprocessing (unless it is optimized).

\subsection{Attributes}
\label{subsec:attributes}

As mentioned, the data set was extracted directly from the internet (please refer to section \ref{sec:solution} for a description of the data extraction and preprocessing). However, we have enriched the data set with website meta-data and statistics from Amazon's Alexa service \cite{alexa}. Further, we had to fetch the PageRank directly from Google. The complete list of attributes in the data set can be found in table \ref{tab:all_attributes} in appendix \ref{apx:attributes}\footnote{Note that the attributes which were fetched from Alexa carry an \texttt{alexa} prefix.}.

%Attributes with the \texttt{has\_content} prefix, indicate whether a website contains content pertaining to a certain category such as {\it technology} or {\it sports}. The \texttt{has\_js} attributes indicate whether a given JavaScript libraries is used by a website. For a description of the remaining attributes, please refer to table \ref{tab:all_attributes}.

In the extracted data set, we found that certain attributes were almost static across the whole data set and decided to exclude these from the actual mining process, as they provided no further insights about the data. This selection process was performed manually, and reduced the final data set to 29 attributes, which yields 70325 values of which 1874 (2.55\%) are missing.

Further, the reduced data set was grouped into three logical subsets, each accounting for some specific aspect of the websites. These final subsets can be seen in table \ref{tab:cluster_attributes}. The subsets, and not the entire data set, were used for the data mining process, as the entire set provided no useful results.

\htab{l l}
{
\toprule
Subset & Attributes in subset\\
\midrule
Rank subset & \texttt{alexa\_load\_time}, \texttt{alexa\_rank}, \texttt{alexa\_rank\_dk}, \\
& \texttt{alexa\_links\_in}, \texttt{internal\_links\_count}, \\
& \texttt{external\_links\_count}, \texttt{page\_rank}, \texttt{title\_tag}, \\
& \texttt{has\_description}, \texttt{has\_keywords}, \texttt{img\_count} \\
\midrule
Technology subset & \texttt{html5}, \texttt{html5\_tags}, \texttt{has\_js\_jquery}, \\
& \texttt{server}, \texttt{cms}, \texttt{has\_analytics} \\
\midrule
Content subset & \texttt{has\_content\_business}, \texttt{has\_content\_film}, \\
& \texttt{has\_content\_food}, \texttt{has\_content\_games}, \\
& \texttt{has\_content\_health}, \texttt{has\_content\_music}, \\
& \texttt{has\_content\_news}, \texttt{has\_content\_shop}, \\
& \texttt{has\_content\_sport}, \texttt{has\_content\_technology}, \\
& \texttt{has\_content\_transport}, \texttt{has\_content\_xxx} \\
\bottomrule
}{The different subsets used for in clustering process.}{tab:cluster_attributes}

%\paragraph{Motivation}
%In this project we have been highly motivated by the fact that we have had the chance to work on a data mining project that is relevant for a real-world company. We thank Statistico for their time involving us in their work, and the resources and time they have invested in getting the project underway.

%\todo{Remove this motivation?}

%\subsection{Overview}
%In the next section we formalize the problem at hand, and discuss our approach to the data mining process. In section \ref{sec:solution} the solution and, in particular, the preprocessing step is described. In section \ref{sec:evaluation} we analyse the extracted data set and discuss the results. Finally, in section \ref{sec:conclusion} we summarize and conclude on the findings.