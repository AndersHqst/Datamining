\section{Analysis}
\label{sec:analysis}

Obviously, generating the data set is not that interesting in itself, and in this section we analyze the data. To get to know the data better, we have automated the process of extracting basic statistics and distributions from the data, of which the most significant are discussed in the following.

Further, we have used clustering algorithms to try and identify any obvious clusters in the data, which turns out to be hard. Finally we test a few classification and prediction algorithms to see if we can predict attributes of interest.

\subsection{Statistics}
\label{subsec:statistics}
To get a better understanding of the data, we have automated to process of generating some basic statistics and overview of the data. We plot every attribute on histograms or bar charts, along the minimum, maximum, mode, standard deviation, and median for numerical values. All non binary attributes are plotted in bins of 10\% intervals of the data and top and bottom 10 values are listed along the plot. These are also plotted in their inter-quantile range as outliers often give a skewed picture. All plots can be found in Appendix \ref{apn:basic_statistics}. As an example we can see that the JavaScript libraries we look for are very uncommon, except JQuery which is on 1 out of 3 sites. And for servers we find that the distribution is much as we would expect, and that we are often able to identified these. Servers binned to ignore version numbers is show in figure \ref{fig:servers_top_10_discrete}.

\hfig{figures/basic_statistics/servers_top_10_discrete.png}{0.8}{Servers Top 10 Discrete}{fig:servers_top_10_discrete}

It turns out that is harder to identify the CMS used by a given site, but as can be seen in figure \ref{fig:cms_distribution_binned_discrete} we can get a distribution that seems very likely. The most doubtful outcome is Drupal, that is known to be very popular, but is often harder to detect by the scanners.

\hfig{figures/basic_statistics/cms_distribution_binned_discrete.png}{0.8}{Cms Distribution Binned}{fig:cms_distribution_binned_discrete}

Another example is the external links count, that is, links from the website pointing to another site. As figure \ref{fig:external_links_count} and \ref{fig:external_links_count_interquantile} show, most sites have less than 106 external links, and the the descending in the number of links is clear in figure \ref{fig:external_links_count_interquantile}

\hfig{figures/basic_statistics/external_links_count.png}{0.8}{External Links Count}{fig:external_links_count}

\hfig{figures/basic_statistics/external_links_count_interquantile.png}{0.8}{External Links Count Interquantile}{fig:external_links_count_interquantile}

An interesting and important stat is the number of back-links to a given website. Table \ref{tab:alexa_links_in_back_links} shows that on the top 10 of sites with most back links, 5 sites are from educational institutions, with ku.dk surpassing even google.dk. A complete list of top and bottom 10 of the numerical attributes can be found in Appendix \ref{apx:tables}.

\hhtab{p{130pt}p{50pt}}
{
\toprule
Site & Links\\
\midrule
ku.dk & 13964\\
google.dk & 11826\\
blogspot.dk & 10921\\
dr.dk & 8617\\
cmsimple.dk & 8369\\
au.dk & 7557\\
ots.dk & 5968\\
ucsj.dk & 5886\\
aau.dk & 5882\\
smartlog.dk & 5558\\
\bottomrule
}{Danish sites with most back-links according to Alexa. Back-links are often considered important for good page rank}{tab:alexa_links_in_back_links}

We find that most of the numbers we get from collecting these statistics, are much as we would expect. This gives us some confidence that we can perform the subsequent data mining process with truthful results. We feel confident that our preprocessing has maintained a certain degree of integrity in the data, and it would be hard to proceed without it.


\subsection{Clustering}
\label{subsec:clustering}

The data was clustered using three different clustering algorithms:

\begin{itemize}
\item K-Means
\item K-Medoids
\item DBSCAN
\end{itemize}

K-Means and K-Medoids were configured to look for 8 clusters\footnote{The choice of 8 clusters were based on experimentation and seemed to yield the most defined clusters.}. A mixed distance measure was used, similar to the one described in the course book \cite{book}. DBSCAN was configured to look for a minimum of 5 points using $\varepsilon = 1.0$.

Three different subsets of attributes were used for the clustering. One subset was chosen to represent the attributes that might influence the ranking of the websites. A second subset focused on the technological attributes. Finally, one subset was created to represent the content of the websites. The specific attributes can be seen in table \ref{tab:cluster_attributes}.

\htab{l l}
{
\toprule
Subset & Attributes in subset\\
\midrule
Rank subset & \texttt{alexa\_load\_time}, \texttt{alexa\_rank}, \texttt{alexa\_rank\_dk}, \\
& \texttt{alexa\_links\_in}, \texttt{internal\_links\_count}, \\
& \texttt{external\_links\_count}, \texttt{page\_rank}, \texttt{title\_tag}, \\
& \texttt{has\_description}, \texttt{has\_keywords}, \texttt{img\_count} \\
\midrule
Technology subset & \texttt{html5}, \texttt{html5\_tags}, \texttt{has\_js\_jquery}, \\
& \texttt{server}, \texttt{cms}, \texttt{has\_analytics} \\
\midrule
Content subset & \texttt{has\_content\_business}, \texttt{has\_content\_film}, \\
& \texttt{has\_content\_food}, \texttt{has\_content\_games}, \\
& \texttt{has\_content\_health}, \texttt{has\_content\_music}, \\
& \texttt{has\_content\_news}, \texttt{has\_content\_shop}, \\
& \texttt{has\_content\_sport}, \texttt{has\_content\_technology}, \\
& \texttt{has\_content\_transport}, \texttt{has\_content\_xxx} \\
\bottomrule
}{The different subsets used for in clustering process.}{tab:cluster_attributes}

\subsubsection{Clustering the rank attributes}

First, we clustered on the rank attributes. Using either of the used algorithms, it was quite easy to spot a few significant clusters. This discussion is based on the clusters found by K-Medoids, but similar pictures could be found using the other algorithms. The two most interesting clusters are analysed below:

\paragraph{The search-engine optimized top websites} 

This cluster has the best average Alexa rank and a very good PageRank. These websites contains both the keyword and description meta tags. The fact that time has been spent on SEO could indicate that this cluster represents websites which use the web as their main platform. This cluster contains sites such as \verb|www.krak.dk|, \verb|www.dba.dk| og \verb|www.amino.dk|.

\paragraph{The unoptimized top websites} 

This cluster has a very good Alexa Rank and PageRank. However, less time has been spent on SEO, which indicates that these websites are the ones that have become popular through some other media and/or do not need this optimization as much as the former cluster. This cluster includes \verb|www.google.dk|, \verb|www.ekstrabladet.dk| og \verb|www.bt.dk|. Obviously, SEO is less important for Google itself.

\paragraph{Other observations} Other clusters can found for the less popular sites, but these were not as distinctly defined as the above mentioned clusters.

\subsubsection{Clustering the technology attributes}

Clustering on the technology related attributes also showed some very distinct clusters. The most interesting clusters are discussed below. Again, the discussion is based on the results of K-Medoids.

\paragraph{High-tech websites}

This cluster primarily consists of the websites which use both \texttt{HTML5} and the JQuery JavaScript library. Although \texttt{HTML5} and JavaScript libraries such as JQuery are becoming more and more common, these websites are the ones that have readily embraced these technologies.

\paragraph{Websites using a CMS}

A specific CMS cluster was found. Although, the CMS usage data in our data set is somewhat underestimated, a very clear grouping of CMS systems was found. Wordpress, Sharepoint and Drupal websites are the most dominant CMS's in this clusters. Further, Google Analytics is used in most of these websites.

\paragraph{Low-tech websites}

Finally, we have the websites which do not use either \texttt{HTML5} or JQuery. K-Medoids actually grouped these websites into two different clusters - a cluster containing low-tech sites running on Microsoft servers and a cluster containing the remaining low-tech sites. The major difference between these is the fact that the Microsoft low-tech cluster still makes use of Google Analytics, whereas this is not the case for the other cluster.

\paragraph{Other observations} 

One interesting observation is the fact that the use of Google Analytics does not seem to be correlated with the general use of technology on the websites. However, this might be due to the fact, that using Google Analytics (or similar services) is often a management decision, whereas management might be somewhat oblivious to the choice technology.

\subsubsection{Clustering the content attributes}

Clustering on the content resulted in a few prominent clusters. The most extreme of these clusters were websites without any content (according to our measure of content, that is) and the websites which satisfied almost all of our content criteria. During this clustering, we noted that the content rich websites had a better Google PageRank.

Due to this, we tried to compare the amount of content in the clusters to their assigned PageRank. This comparison was made by plotting the average PageRanks of the clusters versus the average of the content attributes\footnote{Although these attributes are actually binary, they were treated as numeric, as the average is more useful than the mode in this particular case.}.

As it turns out, there seem to be a relationship between the average amount of content in the cluster and the assigned PageRank. This tendency can be seen in figure \ref{fig:cluster_content} for K-Medoids and in appendix \ref{app:content_pagerank} for K-Means and DBSCAN. Obviously, it must be noted that the exact relationship is still somewhat unclear, but the tendency is definitely present.

\fig{figures/kmedoids_content_pagerank.png}{0.8}{The content attributes clustered using K-Medoids. It is seen, that a large amount of content tends to result in a better PageRank. THIS FIGURE WILL BE IMPROVED IN THE FINAL REPORT.}{fig:cluster_content}

One could also plot the unclustered content attributes against the PageRank. This turned out to give a much more noisy picture, although the same tendencies were found (on average). If course, it should also be noted that the clustering does not really provide any insight which could not be found from the original dataset. However, the clustering helped us \textit{spot} the tendency.

\subsection{Prediction and classification}
\label{subsec:predictclassify}

\subsection{Predicting PageRank}
\label{subsec:predection}
For experimenting with predicting values in the data, we have used the data mining software RapidMiner\footnote{RapidMiner is freely available at http://rapid-i.com/}. RapidMiner provides much of the same functionality as Weka\footnote{Weka is freely available http://www.cs.waikato.ac.nz/ml/weka/}, but in our experience, RapidMiner seems much more user-friendly and does a better job providing documentation and quick help from the workbench.

We want to see if we can predict the page rank of a website, and for this we have set up a process that models linear and polynomial regression, and a neural network\footnote{Appendix \ref{apn:rapidminer_setup} shows examples of the RapidMinerGUI for setting up a data mining process.}. For this process we have used the mixed data set, and replaced missing values with the average of a given attribute. The data set has a total of 2332 missing values with is 2.18\% of the entire set, so we would expect these to have little to no influence on the generalization of our results. Table \ref{tab:prediction_results} lists the results of running predictions on both the Rank subset related subset, and the full set.

\todo{Linear regression Full set CMS 1,3,7,9 had positive coefficients - interesting?}
\hhtab{p{80pt}p{80pt}p{50pt}p{50pt}}
{
\toprule
Model & Data & Error\\
\midrule
Linear Regression & Rank subset & 1.573  \\
Linear Regression & Content subset & 2.640 \\
Linear Regression & Technology subset & 2.510 \\
Linear Regression & Full set & 2.170 \\
Neural Net & Rank subset & 2.875 \\
Neural Net & Technology subset & 2.722 \\
Neural Net & Content subset & 3.036 \\
Neural Net & Full set & 4.587 \\
\bottomrule
}{Overview of model predicting with Page Rank as the target value. The Error is the average squared error after cross validation. Target value is in its original scale.}{tab:prediction_results}

Page rank predictions are based on the cross-validation building-block in RapidMiner, which provides a few strategies like {\it leave-one-out} and and some randomized approaches. As our data is already in a random order we have simply used a linear sampling that separates a standard 10\% as test data, and thus performs 10 runs. As table \ref{tab:prediction_results} indicates we seem to be able to predict the page rank within \(\sqrt{1.573} = 1.2541\) which is also what we would expect, as the Alexa Rank and links related attributes are provided. The above predictions have also been tested with RapidMiner's build in M5 for automatic feature selection, but the results were virtually the same. It seems only a few of the JavaScript related features are excluded.
The output of the neural net model indicates that the Alexa links in attribute is the most important in deciding the page rank. This can be seen by inspecting the importance, or strength, that each attributes has on the outcome of the neural net. The visualization is show in figure \ref{fig:neural_net}

\fig{figures/neural_nets.png}{0.9}{Importance of a connection between neurones in the neural net are indicated by the boldness of the connection. The Alexa links in is in the input layer 7th from the top, the bias is at the bottom. The net on the left is the outcome of learning rate of 0.2 over 500 epocs, and the net on the right is from model with learning rate 0.002 over 5000 epocs.}{fig:neural_net}

The correlation coefficient between page rank and and Alexa links in is 0.2054 with a p value of 0.1808 which supports the above indications. This result is not very surprising, so it could be interesting to see the outcome of models without the Alexa related attributes. We choose the Rank subset minus Alexa related attributes as listed in table \ref{tab:rank_no_alexa}, which yields the results seen in table \ref{tab:rank_no_alexa_prediction}

\hhtab{l l}
{
\toprule
Subset & Attributes in subset\\
\midrule
Rank subset & \texttt{external\_links\_count}, \texttt{page\_rank}, \texttt{title\_tag}, \\
& \texttt{internal\_links\_count}, \texttt{has\_description}, \\
&\texttt{has\_keywords}, \texttt{img\_count} \\
\bottomrule
}{Rank subset without Alexa attributes.}{tab:rank_no_alexa}

\hhtab{p{80pt}p{80pt}p{50pt}p{50pt}}
{
\toprule
Model & Data & Error\\
\midrule
Linear Regression & Rank subset set & 2.808  \\
Neural Net & Rank subset set & 2.924\\
\bottomrule
}{Prediction errors using the subset from table \ref{tab:rank_no_alexa}.}{tab:rank_no_alexa_prediction}

As table \ref{tab:rank_no_alexa_prediction} shows, error has gone up by removing the Alexa related attributes, and shows that we can predict a page rank in the \(\sqrt{2.875} = 1.6955\) range. The interesting thing to say about this is, that its actually not a bad prediction considering the scarce information used. We believe that the connection here is simply, that sites that rank well on Google also manage their websites well, and thus have the above attributes - and not the other way around.

\todo{Try with a classifier on binned page rank}