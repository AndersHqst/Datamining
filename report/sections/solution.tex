\section{Solution}
\label{sec:solution}

We now present our solution, which primarily deals with collection and preprocessing of the data. We have implemented a small preprocessing library that uses a plugin architecture which allows us to add, remove, and alter the way in which we extract features.

\subsection{Preprocessing}
\label{subsec:preprocessing}

% Class-like diagram showing analyser, scanners, website, scraper, calls to alexa.

As mentioned in section \ref{subsec:scraping}, a script was created to download and scrape the websites of interest. The scraped websites (and auxiliary information) were stored on disk for easy access, using a simple and easy to parse data format.

Apart from the websites themselves, auxiliary information was fetched from Alexa and Google. Similar scripts were created to fetch and parse this data. Please refer to the actual source code for more information on this.

A small library was developed to process the raw data (including \texttt{HTML}, \texttt{HTTP} headers, Alexa information, Google PageRank) into a finite set of well defined attributes, which could be used for subsequent data analysis.

A high-level view of the architecture of this preprocessor library can be seen in figure \ref{fig:preprocess}. A specific class (\texttt{Website}) was used to hold the information belonging to each website (including both the raw and parsed \texttt{HTML}). In order to easily extract the desired features, we introduced the concept of \textit{scanners}. Given a \texttt{Website} object, each scanner can extract and preprocess a specific feature from the raw data. This was implemented in a pluggable way, such that scanners could be added/removed as needed. A list of implemented scanners can be found in appendix \ref{app:scanners}\todo{Create this appendix...}.

\dbgfig{preprocess.pdf}{0.9}{A high-level overview of the preprocessing library.}{fig:preprocess}

\subsection{Datamining tools}
\label{subsec:weka}

% How is data fed to weka. Problems?

We decided to use Weka and Rapidminer for the actual data analysis. Weka was used for the initial experimentation, but the richer feature set found in Rapidminer was used for the results presented here. 

In order to use both these tools, the preprocessing library (refer to section \ref{subsec:preprocessing}) was developed such that it could output the data set in the \texttt{ARFF} format, which can be read by both Weka and Rapidminer.

Three versions of the data set were created, in order to support all potential use cases:

\begin{itemize}
\item A version containing the raw values from the data set.
\item A version in which every value was discretised/binned.
\item A combination of the former two, in which only a select few attributes were binned\footnote{These binned attributes were all nominal. In other words, we reduced the number of bins belonging to these attributes. For instance, we grouped all versions of the Apache webserver, instead of considering the specific versions.}.
\end{itemize}

It turned out that the combined version was easiest to work with, and was thus chosen for the actual data analysis.

Although the data set had already been preprocessed, we performed a few extra preprocessing steps using Weka. In order to secure consistency in any distance measures, the numeric attributes were normalised into the domain $[0, 1]$. Further, irrelevant attributes were removed from the dataset\footnote{In the upcoming discussion, we will explicitly state which attributes were used for each specific analysis.}.

