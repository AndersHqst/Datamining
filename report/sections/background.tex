%\section{Background}
%\label{sec:background}
%In this section we present the problem statement, which is based on some interest expressed by Statistico, combined with questions we find relevant to try and answer with different data mining techniques. We also provide a short introduction to technologies used by Statistico and discuss how they can, and cannot, be used to solve the problem at hand.

\subsection{Problem statement}
\label{subsec:problem_statement}
Based on the interests and data of Statistico introduced in secton \ref{sec:introduction}, we want to find out how well we can extract general statistics on websites in the .dk domain. We want to be able to give an overview of the distribution of CMS, server software, and SEO related figures. SEO related data could be related to link counts or presence of certain \texttt{HTML} tags.

In addition, we want to investigate if it is possible to derive interesting correlations or patterns in the meta-data. We will use data mining software to experiment with different clustering algorithms, and see if we can derive any meaningful correlations in the data.

A very important measure for any website is its Alexa-rank and page-rank on Google. We will see if we can predict these values based on the limited data we get. It seems unlikely, or at least not intuitive, that we should be able to derive these numbers from a data mining process that is essentially a simplification of what Alexa and Google's crawlers do, but non the less, the results could be interesting in that it could indicate a small set of parameters to be important for getting a high rank.

%\subsection{Meta data}
%\label{subsec:meta_data}
%We need to decide on a set of attributes that we want to collect from each website. In section \ref{sec:background} we will give an overview of these. Besides the data that we can get directly from the HTML of the websites, we can use Alexa\footnote{By Alexa we refer to Amazon's Alexa Web Information Service, which displays various data figures on websites around the world. http://aws.amazon.com/awis/} to enrich our data set with important figures such as back-link and page load time, which would be hard to get ourselves, but is easy to retrieve as a service.

%\subsection{Accessing the index - Solr and Lucene}
%\label{subsec:solr}
%As Statistico uses Solr, the idea has been to use the Solr querying api. Ideally this would give us a fast way of extracting information from the big data set. However, we decided midway in the process to abandon this idea. Without getting into too much detail, we found that the extra layer of Solr adds unnecessary complexity to the preprocessing step, and argue that have the search engine as a part of the process could ruin some confidence in the data integrity later in the process. As such, we have build our preprocessing up around scraping and parsing the raw HTML from a website. As Statistico stores all the raw HTML as well, our solution can just as easily interface with their data set.

\subsection{Attributes}
\label{subsec:attributes}

The complete list of attributes in the data set can be found in table \ref{tab:all_attributes} in appendix \ref{apx:attributes}. The attributes with the \texttt{alexa} prefix have been downloaded from Alexa and includes the language of the websites, the number of incoming links, the average load time and the global and Danish rank of the websites\footnote{The Alexa rank is an estimate of the actual visitor count rank of the website. For instance, a global Alexa rank of 1 is assigned the most visited website.}. Attributes with the \texttt{has\_content} prefix, indicate whether or not the website contains some specific content (such as technological content). The \texttt{has\_js} attributes indicate whether or not various JavaScript libraries are used. For a description of the remaining attributes, please refer to the table.

As mentioned earlier, irrelevant attributes were removed from the data set, in order to render an analysis possible. Hence, attributes which were more or less static across the whole dataset were removed, as these provided no further insight about the websites. The selection process was performed manually, as this made it possible to individually asses the importance of the different attributes.

Further, the reduced data set was grouped into three logical subsets, each accounting for some specific aspect of the websites. These final subsets can be seen in table \ref{tab:cluster_attributes}. These subsets were used for the actual analysis.

\htab{l l}
{
\toprule
Subset & Attributes in subset\\
\midrule
Rank subset & \texttt{alexa\_load\_time}, \texttt{alexa\_rank}, \texttt{alexa\_rank\_dk}, \\
& \texttt{alexa\_links\_in}, \texttt{internal\_links\_count}, \\
& \texttt{external\_links\_count}, \texttt{page\_rank}, \texttt{title\_tag}, \\
& \texttt{has\_description}, \texttt{has\_keywords}, \texttt{img\_count} \\
\midrule
Technology subset & \texttt{html5}, \texttt{html5\_tags}, \texttt{has\_js\_jquery}, \\
& \texttt{server}, \texttt{cms}, \texttt{has\_analytics} \\
\midrule
Content subset & \texttt{has\_content\_business}, \texttt{has\_content\_film}, \\
& \texttt{has\_content\_food}, \texttt{has\_content\_games}, \\
& \texttt{has\_content\_health}, \texttt{has\_content\_music}, \\
& \texttt{has\_content\_news}, \texttt{has\_content\_shop}, \\
& \texttt{has\_content\_sport}, \texttt{has\_content\_technology}, \\
& \texttt{has\_content\_transport}, \texttt{has\_content\_xxx} \\
\bottomrule
}{The different subsets used for in clustering process.}{tab:cluster_attributes}